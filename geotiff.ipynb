{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Multi-dimensional arrays and datasets\n",
    "import xarray as xr\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Geospatial data analysis\n",
    "import geopandas as gpd\n",
    "\n",
    "# Geospatial operations\n",
    "import rasterio\n",
    "import rioxarray as rio\n",
    "from matplotlib.cm import jet\n",
    "\n",
    "# Filesystem interface to Azure-Datalake Gen1 and Gen2 Storage\n",
    "import adlfs\n",
    "\n",
    "# Import Planetary Computer tools\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TerraClimate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use pystac-client to search the Planetary Computer's STAC API for the subset of the data that we care about, and then we'll load the data directly from Azure Blob Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access STAC catalog and collection.\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace)\n",
    "\n",
    "collection = catalog.get_collection(\"terraclimate\")\n",
    "asset = collection.assets[\"zarr-abfs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open dataset and remove CRS.\n",
    "ds = xr.open_dataset(asset.href,**asset.extra_fields[\"xarray:open_kwargs\"])\n",
    "ds = ds.drop('crs', dim=None) # Remove the CRS coordinate in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimming the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We processes a massive dataset, nearly 2 TB in size, by first trimming it to include only data from November 1, 2017, to November 1, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is a HUGE dataset (nearly 2 TB), we should parse the dataset\n",
    "# Trimming dataset to years 2017 thru 2019\n",
    "ds = ds.sel(time=slice(\"2017-11-01\", \"2019-11-01\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the geographical bounds for southeastern Australia, specifying the minimum and maximum latitude and longitude values. Boolean masks are created to filter the dataset within these bounds, ensuring that only data within the specified latitude and longitude range is retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimming dataset to the desired Lat-Lon bounds (southeastern Australia)\n",
    "\n",
    "min_lon = 139.94\n",
    "min_lat = -39.74\n",
    "max_lon = 151.48\n",
    "max_lat = -30.92\n",
    "\n",
    "mask_lon = (ds.lon >= min_lon) & (ds.lon <= max_lon)\n",
    "mask_lat = (ds.lat >= min_lat) & (ds.lat <= max_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the dataset is cropped to this smaller region using the where method, effectively reducing its size and focusing on the relevant time period and geographical area for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the dataset to smaller Lat-Lon regions\n",
    "ds = ds.where(mask_lon & mask_lat, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing a GeoTIFF File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persist the dataset ds in memory to optimize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist dataset in memory.\n",
    "ds=ds.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the median values along the time dimension, skipping any NaN values, and stores the result in the median variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute median along time dimension.\n",
    "median = ds.median(dim=\"time\", skipna=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define output file name.\n",
    "filename = \"TerraClimate_output.tiff\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions of the file are calculated based on the latitude and longitude dimensions of the median dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the dimensions of the file\n",
    "height = median.dims[\"lat\"]\n",
    "width = median.dims[\"lon\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Coordinate Reference System (CRS) is set to EPSG:4326, which represents common latitude-longitude coordinates. A transformation is defined using the bounding box to ensure the latitude and longitude information is correctly written to the GeoTIFF. The CRS and transformation are written to the median dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Coordinate Reference System (CRS) to be common Lat-Lon coordinates\n",
    "## Define the tranformation using our bounding box so the Lat-Lon information is written to the GeoTIFF\n",
    "gt = rasterio.transform.from_bounds(min_lon,min_lat,max_lon,max_lat,width,height)\n",
    "median.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "median.rio.write_transform(transform=gt, inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GeoTIFF file is created using the defined parameters, with two bands: one for solar radiation (srad) and one for vapor pressure (vap), and the file is compressed using the LZW algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GeoTIFF output file using the defined parameters \n",
    "with rasterio.open(\n",
    "    filename, 'w', driver='GTiff', width=width, height=height,\n",
    "    crs='epsg:4326', transform=gt, count=14, compress='lzw', dtype='float64') as dst:\n",
    "    # Write each variable to a corresponding band\n",
    "    dst.write(median.aet.astype('float64'), 1)      \n",
    "    dst.write(median['def'].astype('float64'), 2)   \n",
    "    dst.write(median.pdsi.astype('float64'), 3)     \n",
    "    dst.write(median.pet.astype('float64'), 4)      \n",
    "    dst.write(median.ppt.astype('float64'), 5)      \n",
    "    dst.write(median.q.astype('float64'), 6)        \n",
    "    dst.write(median.soil.astype('float64'), 7)     \n",
    "    dst.write(median.srad.astype('float64'), 8)     \n",
    "    dst.write(median.swe.astype('float64'), 9)      \n",
    "    dst.write(median.tmin.astype('float64'), 10)    \n",
    "    dst.write(median.tmax.astype('float64'), 11)    \n",
    "    dst.write(median.vap.astype('float64'), 12)     \n",
    "    dst.write(median.vpd.astype('float64'), 13)     \n",
    "    dst.write(median.ws.astype('float64'), 14)     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
